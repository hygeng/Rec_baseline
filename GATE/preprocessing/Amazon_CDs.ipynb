{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dir_path = \"/cluster/home/it_stu110/proj/GATE/dataset/data/amazon/music/\"#'./Amazon_Review/CDs/'\n",
    "rating_file = 'ratings_music.csv'#'ratings_CDs_and_Vinyl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHG1GTQZUYNJN</td>\n",
       "      <td>0001393774</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1372723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A1WX42M589VAMQ</td>\n",
       "      <td>0001393774</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1167350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A2UA9KKUQCTEIN</td>\n",
       "      <td>0001501348</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1381017600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id     item_id  rating   timestamp\n",
       "1    AHG1GTQZUYNJN  0001393774     5.0  1372723200\n",
       "10  A1WX42M589VAMQ  0001393774     5.0  1167350400\n",
       "20  A2UA9KKUQCTEIN  0001501348     5.0  1381017600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_user_rating_records():\n",
    "    col_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data_records = pd.read_csv(dir_path + rating_file, sep=',', names=col_names, engine='python')\n",
    "    return data_records\n",
    "# data_records = pd.read_csv(dir_path + rating_file,index_col = 0)\n",
    "# data_records.columns= ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data_records = read_user_rating_records()\n",
    "data_records.head()\n",
    "data_records.iloc[[1, 10, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578597 486360\n"
     ]
    }
   ],
   "source": [
    "print(len(data_records['user_id'].value_counts()), len(data_records['item_id'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425149 457623\n"
     ]
    }
   ],
   "source": [
    "data_records.loc[data_records.rating < 4, 'rating'] = 0\n",
    "data_records.loc[data_records.rating >= 4, 'rating'] = 1\n",
    "data_records = data_records[data_records.rating > 0]\n",
    "print(len(data_records['user_id'].unique()), len(data_records['item_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users with < 10 interactoins are removed\n",
      "items with < 8 interactoins are removed\n",
      "num of users:31668, num of items:24635\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def remove_infrequent_items(data, min_counts=5):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['item_id'].value_counts()\n",
    "    df = df[df[\"item_id\"].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"items with < {} interactoins are removed\".format(min_counts))\n",
    "    # print(df.describe())\n",
    "    return df\n",
    "\n",
    "def remove_infrequent_users(data, min_counts=10):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['user_id'].value_counts()\n",
    "    df = df[df[\"user_id\"].isin(counts[counts >= min_counts].index)]\n",
    "    print(\"users with < {} interactoins are removed\".format(min_counts))\n",
    "    # print(df.describe())\n",
    "    return df\n",
    "\n",
    "filtered_data = remove_infrequent_users(data_records, 10)\n",
    "filtered_data = remove_infrequent_items(filtered_data, 8)\n",
    "print('num of users:{}, num of items:{}'.format(len(filtered_data['user_id'].unique()), len(filtered_data['item_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id\n",
      "B0001GNDN4    8\n",
      "B0000026AN    8\n",
      "B0000026B4    8\n",
      "B00004X0KU    8\n",
      "B000002UJ3    8\n",
      "B000002UIK    8\n",
      "B000002UI8    8\n",
      "B0000026C5    8\n",
      "B00004X0QN    8\n",
      "B00004X0TH    8\n",
      "dtype: int64\n",
      "user_id\n",
      "A02039013W06XH9FVVFUZ    1\n",
      "A151Z5T4K8XK9X           1\n",
      "A3DEWYL97V73PI           1\n",
      "A19PW02C37RFX1           1\n",
      "A3DMPUPJ248CGI           1\n",
      "A21ON7MBZ14C8D           1\n",
      "A1513MWN910XKQ           1\n",
      "A3DOCWNJH2NLA5           1\n",
      "A3DU61S18GW7NW           1\n",
      "A3DUY3FHH21T3T           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data.groupby('item_id').size().sort_values(ascending=True)[:10])\n",
    "print(filtered_data.groupby('user_id').size().sort_values(ascending=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0738900370' '0738900672' '0738919039' '0760135002' '0767804341'\n",
      " '0769716903' '0769720994' '0780018664' '0780607287' '0783112793']\n"
     ]
    }
   ],
   "source": [
    "# read item's reviews\n",
    "item_list = filtered_data['item_id'].unique()\n",
    "item_set = set(item_list)\n",
    "\n",
    "print(item_list[:10])\n",
    "\n",
    "review_file = 'Digital_Music.json'#'reviews_CDs_and_Vinyl_5.json.gz'\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "        # yield json.dumps(eval(l))\n",
    "\n",
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "        # yield json.dumps(eval(l))\n",
    "\n",
    "review_dict = dict()  # [review_id] = review_text\n",
    "review_helpful = dict()\n",
    "for l in parse(dir_path + review_file):\n",
    "    if l['asin'] in item_set:\n",
    "        if l['asin'] in review_dict:\n",
    "            if l['helpful'][0] / float(l['helpful'][1] + 0.01) > review_helpful[l['asin']] and len(l['reviewText']) > 10:\n",
    "                review_dict[l['asin']] = l['reviewText']\n",
    "                review_helpful[l['asin']] = l['helpful'][0] / float(l['helpful'][1] + 0.01)\n",
    "        else:\n",
    "            if len(l['reviewText']) > 10:\n",
    "                review_dict[l['asin']] = l['reviewText']\n",
    "                review_helpful[l['asin']] = l['helpful'][0] / float(l['helpful'][1] + 0.01)\n",
    "\n",
    "review_dict = dict()  # [review_id] = review_text\n",
    "review_helpful = dict()\n",
    "for l in parse(dir_path + review_file):\n",
    "    if l['asin'] in item_set:\n",
    "        if (l['asin'] in review_dict) and 'vote' in l.keys() and 'reviewText' in l.keys():\n",
    "            if l['vote']  > review_helpful[l['asin']] and len(l['reviewText']) > 10:\n",
    "                review_dict[l['asin']] = l['reviewText']\n",
    "                review_helpful[l['asin']] = l['vote']\n",
    "        else:\n",
    "            if 'reviewText' in l.keys():\n",
    "                if len(l['reviewText']) > 10 and 'vote' in l.keys():\n",
    "                    review_dict[l['asin']] = l['reviewText']\n",
    "                    review_helpful[l['asin']] = l['vote'][0] \n",
    "# print review_dict['1300966947']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "24635\n"
     ]
    }
   ],
   "source": [
    "# delete items without reviews\n",
    "item_without_review = []\n",
    "for item_id in item_list:\n",
    "    if item_id not in review_dict:\n",
    "        item_without_review.append(item_id)\n",
    "\n",
    "print(item_without_review)\n",
    "\n",
    "for item_id in item_without_review:\n",
    "    filtered_data = filtered_data[filtered_data['item_id'] != item_id]\n",
    "\n",
    "item_list = filtered_data['item_id'].unique()\n",
    "print(len(item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id, review in review_dict.items():\n",
    "    if len(review) < 5:\n",
    "        print(item_id)\n",
    "# print review_dict['B002IUAUI2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('succressfully created sequencial data! head:', user_id\n",
      "A02039013W06XH9FVVFUZ                                         [B000008UGB]\n",
      "A0644664269UTSPKMVCVH    [B000003G7W, B000009HQW, B000002IIG, B0000AINI...\n",
      "A08161909WK3HU7UYTMW     [B00005OMGE, B0041WLBEC, B000069JJF, B000002MH...\n",
      "A099280716ZEH5UPWAN4A    [B0000C23DW, B000005B0U, B009NOVCSS, B00000136...\n",
      "A1002VY75YRZYF                                    [B000003JAH, B000002BMD]\n",
      "Name: item_id, dtype: object)\n",
      "user_id\n",
      "A0644664269UTSPKMVCVH    [B000003G7W, B000009HQW, B000002IIG, B0000AINI...\n",
      "A08161909WK3HU7UYTMW     [B00005OMGE, B0041WLBEC, B000069JJF, B000002MH...\n",
      "A099280716ZEH5UPWAN4A    [B0000C23DW, B000005B0U, B009NOVCSS, B00000136...\n",
      "A1004AX2J2HXGL           [B00008J4P5, B000005JAC, B000007MVM, B000077VQ...\n",
      "A1006V961PBMKA           [B000003TAW, B000065BW8, B00006V9A0, B00008J4P...\n",
      "A100G9PJZUCJQW           [B00000ICO0, B000003494, B001E2PTJK, B000075AJ...\n",
      "A100JCBNALJFAW           [B0001NBMBC, B00000099X, B0007WF1X2, B000BEZPQ...\n",
      "A100T19LDH0GGN           [B000002P7Z, B00004R84T, B00004YMP4, B00004S7Z...\n",
      "A100TF7VLG8RBV           [B000001FXX, B00002MYYI, B00001QGPF, B000003QG...\n",
      "A10127132IE1A73IN1HGO    [B0052V0NQ8, B004T4YPI6, B004ZBIJE4, B007QUZKP...\n",
      "Name: item_id, dtype: object\n",
      "24934\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# convert records to sequential data per user\n",
    "def convert_data(data):\n",
    "    # for each user, sort by timestamps\n",
    "    df = deepcopy(data)\n",
    "    df_ordered = df.sort_values(['timestamp'], ascending=True)\n",
    "    data = df_ordered.groupby('user_id')['item_id'].apply(list)\n",
    "    #print(data)\n",
    "    #time_l = df_ordered.groupby('user')['checkin_time'].apply(list)\n",
    "    #print(time_l)\n",
    "    print(\"succressfully created sequencial data! head:\", data.head(5))\n",
    "    unique_data = df_ordered.groupby('user_id')['item_id'].nunique()\n",
    "    data = data[unique_data[unique_data >= 5].index]\n",
    "    print(data[:10])\n",
    "    print(len(data))\n",
    "    return data\n",
    "\n",
    "seq_data = convert_data(filtered_data)\n",
    "print(type(seq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24934 24634\n"
     ]
    }
   ],
   "source": [
    "user_item_dict = seq_data.to_dict()\n",
    "user_mapping = []\n",
    "item_set = set()\n",
    "for user_id, item_list in seq_data.items():\n",
    "    user_mapping.append(user_id)\n",
    "    for item_id in item_list:\n",
    "        item_set.add(item_id)\n",
    "item_mapping = list(item_set)\n",
    "\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15915, 2807, 9857, 9850, 19895, 9988, 23902], [5450, 2440, 14127, 8223, 22719, 21934, 15795, 23756, 3482, 9481, 3170, 21779, 2587, 15610, 18908, 16178, 361, 190, 5849, 767, 4487, 4178, 3480, 2064, 2092, 5659, 9573, 11301, 20098], [5547, 5761, 17353, 6837, 10934, 20543, 7669, 2092, 18239], [19975, 13670, 3652, 8475, 1490], [3378, 20231, 526, 19975, 22281, 1820, 574, 16703, 7023, 14325, 24508, 19701, 8790]]\n"
     ]
    }
   ],
   "source": [
    "def generate_inverse_mapping(data_list):\n",
    "    inverse_mapping = dict()\n",
    "    for inner_id, true_id in enumerate(data_list):\n",
    "        inverse_mapping[true_id] = inner_id\n",
    "    return inverse_mapping\n",
    "\n",
    "def convert_to_inner_index(user_records, user_mapping, item_mapping):\n",
    "    inner_user_records = []\n",
    "    user_inverse_mapping = generate_inverse_mapping(user_mapping)\n",
    "    item_inverse_mapping = generate_inverse_mapping(item_mapping)\n",
    "    for user_id in range(len(user_mapping)):\n",
    "        real_user_id = user_mapping[user_id]\n",
    "        item_list = list(user_records[real_user_id])\n",
    "        for index, real_item_id in enumerate(item_list):\n",
    "            item_list[index] = item_inverse_mapping[real_item_id]\n",
    "        inner_user_records.append(item_list)\n",
    "    return inner_user_records, user_inverse_mapping, item_inverse_mapping\n",
    "\n",
    "inner_data_records, user_inverse_mapping, item_inverse_mapping = convert_to_inner_index(user_item_dict, user_mapping, item_mapping)\n",
    "print(inner_data_records[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def generate_rating_matrix(train_set, num_users, num_items):\n",
    "    # three lists are used to construct sparse matrix\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for user_id, article_list in enumerate(train_set):\n",
    "        for article in article_list:\n",
    "            row.append(user_id)\n",
    "            col.append(article)\n",
    "            data.append(1)\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    data = np.array(data)\n",
    "    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n",
    "    return rating_matrix\n",
    "\n",
    "rating_matrix = generate_rating_matrix(inner_data_records, len(user_mapping), len(item_mapping))\n",
    "rating_matrix = rating_matrix.transpose()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "relation_matrix = cosine_similarity(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007782956683325232"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.nnz / float(len(user_mapping) * len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "11751\n",
      "0.948683298051\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "np.fill_diagonal(relation_matrix, 0)\n",
    "threshold = 0.3\n",
    "max_count = 0\n",
    "for i in range(len(item_mapping)):\n",
    "    max_count = max(np.count_nonzero((relation_matrix[i] >= threshold) == True), max_count)\n",
    "    \n",
    "print(max_count)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(item_mapping)):\n",
    "    if np.count_nonzero((relation_matrix[i] >= threshold) == True) > 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "print(np.max(relation_matrix))\n",
    "print(relation_matrix[0])\n",
    "print(relation_matrix[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24934 24634\n"
     ]
    }
   ],
   "source": [
    "relation_matrix[relation_matrix < threshold] = 0\n",
    "relation_matrix[relation_matrix > 0] = 1\n",
    "relation_matrix = csr_matrix(relation_matrix)\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi chart past month think predict would ciara f missi elliot step cent disco inferno snoop dogg f pharrel drop like hot jennif lopez get right nelli f tim mcgraw game f cent lenni kravitz ladi ludacri get back destini child soldier lose breath u vertigo killer somebodi told mr brightsid gwen stefani rich girl wait john mayer daughter eminem like toy soldier lose twista f faith evan hope modest mous float usher boo caught good charlott wan na live ashanti u ja rule f r kelli ashanti wonder lloyd bank f avant karma jadakiss f mariah carey u make wan na lindsay lohan\n"
     ]
    }
   ],
   "source": [
    "# process review content\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# generate the whole document\n",
    "all_review = []\n",
    "for item_id in item_mapping:\n",
    "    all_review.append([review_dict[item_id]])\n",
    "\n",
    "# use nltk to remove stopwords, and stemming each word\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "\n",
    "review_str = []\n",
    "for i, movie in enumerate(all_review):\n",
    "    # Use regular expressions to do a find-and-replace\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",  # The pattern to search for\n",
    "                          \" \",  # The pattern to replace it with\n",
    "                          movie[0])  # The text to search\n",
    "    # print letters_only\n",
    "    letters_only = letters_only.lower()\n",
    "    tokens = nltk.word_tokenize(letters_only)\n",
    "    tokens = [w for w in tokens if w.lower() not in stopwords_set]\n",
    "    # print tokens\n",
    "    porter = [porter_stemmer.stem(t) for t in tokens]\n",
    "    # print porter\n",
    "    all_review[i] = porter\n",
    "    review_str.append(' '.join(porter))\n",
    "\n",
    "print(review_str[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24341\n",
      "139\n",
      "1\n",
      "24634\n"
     ]
    }
   ],
   "source": [
    "# convert to bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, min_df=3)\n",
    "word_counts = vectorizer.fit_transform(review_str)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(vocab))\n",
    "print(word_counts.data.max())\n",
    "print(word_counts.data.min())\n",
    "print(len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007782956683325232"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.nnz / float(len(user_mapping) * len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store bag-of-words to file\n",
    "data_dir = \"/cluster/home/it_stu110/proj/GATE/dataset/data/amazon/music/\"\n",
    "def vocabulary_to_file(vocab):\n",
    "    f0 = open(data_dir + 'vocabulary.txt', 'w')\n",
    "    for word in vocab:\n",
    "        f0.write(word + '\\n')\n",
    "    f0.close()\n",
    "\n",
    "\n",
    "def word_count_to_file(item_list, word_count):\n",
    "    f0 = open(data_dir + 'word_counts.txt', 'w')\n",
    "    for i, document in enumerate(word_count):\n",
    "        indices = document.indices\n",
    "        counts = document.data\n",
    "        num_words = document.count_nonzero()\n",
    "        f0.write(str(item_list[i]) + ' ' + str(num_words))\n",
    "        for j, indice in enumerate(indices):\n",
    "            f0.write(' ' + str(indice) + ':' + str(counts[j]))\n",
    "        f0.write('\\n')\n",
    "    f0.close()\n",
    "\n",
    "vocabulary_to_file(vocab)\n",
    "word_count_to_file(item_mapping, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(user_item_dict, data_dir +'Electronics_user_records')\n",
    "save_obj(user_mapping, data_dir +'Electronics_user_mapping')\n",
    "save_obj(item_mapping, data_dir +'Electronics_item_mapping')\n",
    "save_obj(relation_matrix, data_dir +'item_relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aa', u'aaa', u'aaaah', u'aacut', u'aad', u'aah', u'aaliyah', u'aaron', u'ab', u'abacab']\n",
      "[u'studi', u'chart', u'past', u'month', u'think', u'predict', u'would', u'ciara', u'f', u'missi', u'elliot', u'step', u'cent', u'disco', u'inferno', u'snoop', u'dogg', u'f', u'pharrel', u'drop', u'like', u'hot', u'jennif', u'lopez', u'get', u'right', u'nelli', u'f', u'tim', u'mcgraw', u'game', u'f', u'cent', u'lenni', u'kravitz', u'ladi', u'ludacri', u'get', u'back', u'destini', u'child', u'soldier', u'lose', u'breath', u'u', u'vertigo', u'killer', u'somebodi', u'told', u'mr', u'brightsid', u'gwen', u'stefani', u'rich', u'girl', u'wait', u'john', u'mayer', u'daughter', u'eminem', u'like', u'toy', u'soldier', u'lose', u'twista', u'f', u'faith', u'evan', u'hope', u'modest', u'mous', u'float', u'usher', u'boo', u'caught', u'good', u'charlott', u'wan', u'na', u'live', u'ashanti', u'u', u'ja', u'rule', u'f', u'r', u'kelli', u'ashanti', u'wonder', u'lloyd', u'bank', u'f', u'avant', u'karma', u'jadakiss', u'f', u'mariah', u'carey', u'u', u'make', u'wan', u'na', u'lindsay', u'lohan']\n",
      "studi chart past month think predict would ciara f missi elliot step cent disco inferno snoop dogg f pharrel drop like hot jennif lopez get right nelli f tim mcgraw game f cent lenni kravitz ladi ludacri get back destini child soldier lose breath u vertigo killer somebodi told mr brightsid gwen stefani rich girl wait john mayer daughter eminem like toy soldier lose twista f faith evan hope modest mous float usher boo caught good charlott wan na live ashanti u ja rule f r kelli ashanti wonder lloyd bank f avant karma jadakiss f mariah carey u make wan na lindsay lohan\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(all_review[-1])\n",
    "print(review_str[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20664, 3552, 15706, 14019, 21551, 16577, 24030, 3779, 13844, 6608, 20430, 3431, 5737, 10594, 19770, 5934, 16009, 6191, 12315, 10073, 11127, 12529, 8568, 17916, 14527, 21710, 13308, 8391, 3431, 12164, 11763, 11866, 12666, 8568, 1420, 5476, 3650, 19847, 12547, 2625, 23122, 11574, 19880, 21814, 14191, 2679, 9228, 20414, 17869, 8666, 23409, 11220, 13245, 5108, 6675, 12315, 21941, 19847, 12547, 22331, 7332, 7043, 10030, 13923, 14166, 7806, 22921, 2396, 3357, 8831, 3545, 23448, 14361, 12416, 1125, 10968, 18269, 11486, 1125, 23954, 12433, 1558, 1333, 11420, 10988, 13046, 3206, 12888, 23448, 14361, 12355, 12475]\n"
     ]
    }
   ],
   "source": [
    "word_to_index = dict()\n",
    "for w_id, word in enumerate(vocab):\n",
    "    word_to_index[word] = w_id\n",
    "\n",
    "all_review_index = []\n",
    "for i in range(len(review_str)):\n",
    "    cur_review = review_str[i].split(' ')\n",
    "    cur_index = []\n",
    "    for word in cur_review:\n",
    "        if word in word_to_index:\n",
    "            cur_index.append(word_to_index[word])\n",
    "    all_review_index.append(cur_index)\n",
    "    \n",
    "print(all_review_index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store word sequence to a file\n",
    "save_obj(all_review_index, 'review_word_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B000FBFTD2', 'B000005S5N', 'B000XPU5NY', 'B000007O09', 'B002A9OJ0Q', 'B001PNVYMO', 'B000002L7Y', 'B000Y35A94', 'B006CS7UXO', 'B003U0I9U0', 'B00004SYWZ', 'B0044WWLMU', 'B000001EMN', 'B000024T5L']\n",
      "['B000FBFTD2', 'B000005S5N', 'B000XPU5NY', 'B000007O09', 'B002A9OJ0Q', 'B001PNVYMO', 'B000002L7Y', 'B000Y35A94', 'B006CS7UXO', 'B003U0I9U0', 'B00004SYWZ', 'B0044WWLMU', 'B000001EMN', 'B000024T5L']\n",
      "[91, 1957, 3300, 3737, 7260, 11032, 11439, 13192, 15231, 17233, 20633, 22262, 22414, 23704]\n"
     ]
    }
   ],
   "source": [
    "print(seq_data[-1])\n",
    "user_inverse_mapping = generate_inverse_mapping(user_mapping)\n",
    "item_inverse_mapping = generate_inverse_mapping(item_mapping)\n",
    "print(user_item_dict[user_mapping[-1]])\n",
    "tmp = []\n",
    "for item_id in seq_data[-1]:\n",
    "    tmp.append(item_inverse_mapping[item_id])\n",
    "print(sorted(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'studi', u'chart', u'past', u'month', u'think', u'predict', u'would', u'ciara', u'f', u'missi', u'elliot', u'step', u'cent', u'disco', u'inferno', u'snoop', u'dogg', u'f', u'pharrel', u'drop', u'like', u'hot', u'jennif', u'lopez', u'get', u'right', u'nelli', u'f', u'tim', u'mcgraw', u'game', u'f', u'cent', u'lenni', u'kravitz', u'ladi', u'ludacri', u'get', u'back', u'destini', u'child', u'soldier', u'lose', u'breath', u'u', u'vertigo', u'killer', u'somebodi', u'told', u'mr', u'brightsid', u'gwen', u'stefani', u'rich', u'girl', u'wait', u'john', u'mayer', u'daughter', u'eminem', u'like', u'toy', u'soldier', u'lose', u'twista', u'f', u'faith', u'evan', u'hope', u'modest', u'mous', u'float', u'usher', u'boo', u'caught', u'good', u'charlott', u'wan', u'na', u'live', u'ashanti', u'u', u'ja', u'rule', u'f', u'r', u'kelli', u'ashanti', u'wonder', u'lloyd', u'bank', u'f', u'avant', u'karma', u'jadakiss', u'f', u'mariah', u'carey', u'u', u'make', u'wan', u'na', u'lindsay', u'lohan']\n"
     ]
    }
   ],
   "source": [
    "print(all_review[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(word_counts.shape[0]):\n",
    "    if word_counts.getrow(i).getnnz() == 0:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}